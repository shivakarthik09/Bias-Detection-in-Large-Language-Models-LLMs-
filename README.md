# Bias Detection in Large Language Models (LLMs)

## Overview
This project aims to detect and mitigate biases in text generated by Large Language Models (LLMs) such as GPT-2 and BERT. The focus is on identifying biases across key demographic factors (gender, race, age, and socioeconomic status) to improve model fairness and reduce harmful stereotypes. The project also includes multilingual analysis, particularly exploring biases in Indian languages to examine the intersection of language, culture, and AI model performance.

## Key Features
- **Bias Detection:** Utilizes AIF360 and FairSeq libraries to identify biases in LLM-generated text.
- **Model Comparison:** Compares multiple pre-trained models (GPT-2, BERT) for bias robustness.

## Technologies Used
- **AIF360:** IBMâ€™s AI Fairness 360 toolkit for detecting and mitigating bias in AI models.
- **FairSeq:** A sequence-to-sequence learning toolkit for training and fine-tuning models.
- **GPT-2 & BERT:** Pre-trained language models used for generating text and testing bias mitigation strategies.
- **Python:** Core language for implementing the bias detection system.

## Future Scope
- **Bias Visualization:** Visualizes bias patterns across different demographic factors, improving transparency and trustworthiness of models.
- **Multilingual Analysis:** Includes an in-depth study of Indian languages to explore cultural biases in text generation.
